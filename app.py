import streamlit as st
import joblib
import numpy as np
import re
import nltk
import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def preprocess(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|@\S+", "", text)
    text = re.sub(r"[^a-z\s]", "", text)
    tokens = text.split()
    cleaned = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(cleaned)

# åŠ è½½æ¨¡å‹
tfidf = joblib.load("tfidf_vectorizer.pkl")
lr_model = joblib.load("logistic_model.pkl")
nb_model = joblib.load("naive_bayes_model.pkl")
svm_model = joblib.load("svm_model.pkl")

model_dict = {
    "Logistic Regression": lr_model,
    "Naive Bayes": nb_model,
    "SVM": svm_model,
    "æ··åˆæ¨¡å‹ï¼ˆLR + NBï¼‰": None
}

# ğŸ§  é¡µé¢è®¾ç½®
st.set_page_config(page_title="å‡æ–°é—»è¯†åˆ«ç³»ç»Ÿ", layout="centered")
st.title("ğŸ“° å‡æ–°é—»æ™ºèƒ½è¯†åˆ«ç³»ç»Ÿ")
st.markdown("é€šè¿‡æœºå™¨å­¦ä¹ åˆ¤æ–­ä¸€æ®µæ–°é—»æ˜¯çœŸå®è¿˜æ˜¯è™šå‡ï¼Œå¹¶æ ‡å‡ºæ¨¡å‹é‡ç‚¹å…³æ³¨çš„è¯æ±‡ã€‚")

# âœï¸ æ–‡æœ¬è¾“å…¥æ¡†
user_input = st.text_area("ğŸ“ è¯·è¾“å…¥ä½ è¦æ£€æµ‹çš„æ–°é—»å†…å®¹ï¼š", height=200, help="å¯è¾“å…¥å®Œæ•´æ­£æ–‡æˆ–ç®€è¦æ ‡é¢˜")

# æ¨¡å‹é€‰æ‹©ï¼ˆé»˜è®¤é€‰æ··åˆï¼‰
model_option = st.selectbox("ğŸ¤– é€‰æ‹©åˆ†ç±»æ¨¡å‹ï¼š", list(model_dict.keys()), index=3)

# ğŸš€ æŒ‰é’®è§¦å‘
if st.button("å¼€å§‹æ£€æµ‹"):

    if not user_input.strip():
        st.warning("è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬åå†å¼€å§‹æ£€æµ‹ã€‚")
    else:
        cleaned_input = preprocess(user_input)
        x_input = tfidf.transform([cleaned_input])

        # æ¨¡å‹æ¨ç†
        if model_option == "æ··åˆæ¨¡å‹ï¼ˆLR + NBï¼‰":
            prob_lr = lr_model.predict_proba(x_input)[0][1]
            prob_nb = nb_model.predict_proba(x_input)[0][1]
            fake_prob = (prob_lr + prob_nb) / 2
        else:
            model = model_dict[model_option]
            if hasattr(model, "predict_proba"):
                fake_prob = model.predict_proba(x_input)[0][1]
            else:
                decision = model.decision_function(x_input)
                fake_prob = 1 / (1 + np.exp(-decision))[0]

        # ğŸ§¾ åˆ¤å®šç»“æœ
        label = "ğŸ”´ å‡æ–°é—»" if fake_prob >= 0.5 else "ğŸŸ¢ çœŸå®æ–°é—»"
        if fake_prob >= 0.5:
            st.error(f"### ğŸ§¾ åˆ¤å®šç»“æœï¼š{label}")
        else:
            st.success(f"### ğŸ§¾ åˆ¤å®šç»“æœï¼š{label}")

        # ğŸ¯ æ˜¾ç¤ºé¢„æµ‹æ¦‚ç‡è¿›åº¦æ¡
        st.markdown("#### ğŸ“Š å‡æ–°é—»æ¦‚ç‡é¢„æµ‹")
        st.progress(int(fake_prob * 100))
        st.metric("æ¨¡å‹åˆ¤æ–­è¯¥æ–‡æœ¬ä¸ºå‡æ–°é—»çš„æ¦‚ç‡", f"{fake_prob * 100:.2f}%")

        # ğŸ” å…³é”®è¯æå– + é«˜äº®
        feature_names = tfidf.get_feature_names_out()
        input_vector = x_input.toarray()[0]
        top_indices = input_vector.argsort()[::-1][:10]
        highlight_words = [feature_names[i] for i in top_indices if input_vector[i] > 0]

        st.markdown("#### ğŸ§  æ¨¡å‹å…³æ³¨å…³é”®è¯ï¼š")
        st.write(", ".join(highlight_words))

        # âœ¨ åŸæ–‡å…³é”®è¯æŸ“è‰²æ˜¾ç¤º
        highlighted_text = user_input
        for word in highlight_words:
            pattern = re.compile(rf"(?i)\b{re.escape(word)}\b")
            highlighted_text = pattern.sub(
                f"<span style='background-color:#ffcccc; color:#c00'><b>{word}</b></span>",
                highlighted_text
            )

        st.markdown("#### ğŸ§  å…³é”®è¯é«˜äº®åŸæ–‡å±•ç¤ºï¼ˆä»¿ AI æ£€æµ‹å·¥å…·ï¼‰", unsafe_allow_html=True)
        st.markdown(highlighted_text, unsafe_allow_html=True)

        # ğŸ’¡ å±•ç¤ºæç¤º
        st.caption("å…³é”®è¯ä»…åŸºäº TF-IDF æå–ï¼Œå¹¶ä¸ä»£è¡¨æœ€ç»ˆåˆ¤å®šä¾æ®ï¼Œä»…ä¾›å‚è€ƒã€‚")
